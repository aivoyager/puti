"""
@Author: obstacles
@Time:  2025-04-09 16:35
@Description:  
"""
import re
import asyncio

from abc import ABC
from llm.tools import BaseTool, ToolArgs
from pydantic import ConfigDict, Field
from llm.nodes import OllamaNode, LLMNode
from conf.llm_config import LlamaConfig
from logs import logger_factory

lgr = logger_factory.llm


class GenerateCzArgs(ToolArgs):
    topic: str = Field(description='Whether the generated tweets need to be related to a hot topic.')


class GenerateCzTweet(BaseTool, ABC):
    model_config = ConfigDict(arbitrary_types_allowed=True, extra="allow")

    name: str = 'generate_cz_tweet'
    desc: str = 'Use this tool to generate a cz tweet can be posted on twitter.'
    topic: GenerateCzArgs = None

    async def run(self, topic='', *args, **kwargs):
        llm: LLMNode = kwargs.get('llm')
        prompt = """
Below are instructions that describe the task, along with input that provides more context.
Write a response that completes the request appropriately.
Before answering, think carefully about the question and create a step-by-step thought chain to ensure that the answer is logical and accurate.

### Instruction
You play a role in the blockchain area called "赵长鹏" （cz or changpeng zhao）. Reply with his accent, 
speak in his habit. He goes by the Twitter name CZ �� BNB or cz_binance or 大表哥 and is commonly known as cz. 

### Question
User: {}

### Response
Assistant: <think>{}
        """
        topic_exp = f'must related to topic: "{topic}"'
        if topic:
            user_input = f'Come up a tweet as cz {topic_exp}, which tweet characters must between 150 and 200.'
        else:
            user_input = f'Come up a tweet as cz, which tweet characters must between 50 and 240.'
        prompt = prompt.format(user_input, '')
        # ds r1 not recommend system message
        conversation = [{'role': 'user', 'content': prompt}]
        conf = LlamaConfig()
        conf.MODEL = 'cz_14b:tweet'
        node = OllamaNode(llm_name='cz', conf=conf)
        resp = await node.chat(conversation)
        while self.validate_resp(resp, llm)[0] is False:
            lgr.warning(self.validate_resp(resp, llm)[1])
            resp = await node.chat(conversation)
        final_rs = self.validate_resp(resp, llm)[1]
        if final_rs.endswith(','):
            final_rs = final_rs.rstrip(',') + '.'
        return {'generated_tweet': final_rs}

    @staticmethod
    def validate_by_llm(text, llm):
        msg = [{'role': 'system', 'content': 'As a moderator of the content of tweets.'},
               {'role': 'user', 'content': 'Judging from the content whether this is a reasonable tweet, '
                                           'the logic is normal, the English and Chinese expression is normal '
                                           '(full text in Chinese or English), not because of the model illusion'
                                           ' generated by gibberish. And The content of the tweet must be consistent '
                                           'with what Zhao Changpeng (cz、赵长鹏) might post otherwise it is'
                                           'not normal. If normal, '
                                           'only "1" is returned, if abnormal, '
                                           f'only "0" is returned.\n tweet: {text}'}]
        loop = asyncio.get_event_loop()
        resp = loop.run_until_complete(llm.chat(msg))
        if resp == '1':
            return True
        else:
            return False

    def validate_resp(self, text, llm):
        match = re.search(r'</think>(.*)', text) or re.search(r'(?<=Assistant:\s)(.*)', text, re.DOTALL)
        if match:
            length = len(match.group(1).strip())
            if length > 240 or length < 50:
                return False, f'Generate tweet len is invalid. origin: {text}'
            if self.validate_by_llm(text, llm) is False:
                return False, f'llm think its invalid. origin: {text}'
            return True, match.group(1).strip()
        else:
            has_think = bool(re.search(r'<think>', text))
            has_think_end = bool(re.search(r'</think>', text))
            if not has_think and not has_think_end:
                if len(text.strip()) > 270 or len(text.strip()) < 50:
                    return False, f'Generate tweet len is invalid. origin: {text}'
                if self.validate_by_llm(text, llm) is False:
                    return False, f'llm think its invalid. origin: {text}'
                return True, text.strip()
            return False, f'The </think> tag is missing in the text. origin: {text}'
